"""Tests for exploitability_validation package."""

import os
import json
import tempfile
import pytest
from pathlib import Path

from packages.exploitability_validation import (
    build_checklist,
    extract_functions,
    detect_language,
    update_checklist_coverage,
    get_coverage_stats,
    validate_checklist,
    validate_findings,
    create_empty_checklist,
    create_empty_findings,
    create_finding,
    run_validation,
    PipelineConfig,
    PipelineState,
    ValidationOrchestrator,
    Stage,
    StageStatus,
    StageResult,
    LANGUAGE_MAP,
    DEFAULT_EXCLUDES,
    convert_sarif_data,
)
from packages.exploitability_validation.checklist_builder import (
    should_exclude,
    is_binary_file,
    is_generated_file,
    GENERATED_MARKERS,
)
from packages.llm_analysis.agent import convert_validated_to_agent_format


class TestLanguageDetection:
    def test_python(self):
        assert detect_language("foo.py") == "python"

    def test_javascript(self):
        assert detect_language("bar.js") == "javascript"
        assert detect_language("bar.jsx") == "javascript"

    def test_typescript(self):
        assert detect_language("baz.ts") == "typescript"
        assert detect_language("baz.tsx") == "typescript"

    def test_c(self):
        assert detect_language("main.c") == "c"
        assert detect_language("header.h") == "c"

    def test_unknown(self):
        assert detect_language("unknown.xyz") is None


class TestFunctionExtraction:
    def test_python_functions(self):
        content = '''
def foo():
    pass

async def bar(x, y):
    return x + y

def _private():
    pass
'''
        functions = extract_functions("test.py", "python", content)
        names = [f.name for f in functions]
        assert "foo" in names
        assert "bar" in names
        assert "_private" in names
        assert len(functions) == 3

    def test_javascript_functions(self):
        content = '''
function foo() {}
const bar = () => {};
async function baz() {}
'''
        functions = extract_functions("test.js", "javascript", content)
        names = [f.name for f in functions]
        assert "foo" in names
        assert "bar" in names
        assert "baz" in names

    def test_c_functions(self):
        content = '''
int main(int argc, char *argv[]) {
    return 0;
}

void helper() {
    printf("hello");
}
'''
        functions = extract_functions("test.c", "c", content)
        names = [f.name for f in functions]
        assert "main" in names
        assert "helper" in names

    def test_java_functions(self):
        content = '''
public class MyClass {
    public void doSomething() {
        System.out.println("hello");
    }

    private static int calculate(int x) {
        return x * 2;
    }
}
'''
        functions = extract_functions("Test.java", "java", content)
        names = [f.name for f in functions]
        assert "doSomething" in names
        assert "calculate" in names

    def test_go_functions(self):
        content = '''
package main

func main() {
    fmt.Println("hello")
}

func (s *Server) handleRequest(w http.ResponseWriter, r *http.Request) {
    // method with receiver
}

func helper() int {
    return 42
}
'''
        functions = extract_functions("main.go", "go", content)
        names = [f.name for f in functions]
        assert "main" in names
        assert "handleRequest" in names
        assert "helper" in names

    def test_python_syntax_error_fallback(self):
        # Invalid Python syntax - should fall back to regex
        content = '''
def valid_func():
    pass

def another_func(x, y:
    # syntax error - missing closing paren
    pass
'''
        functions = extract_functions("broken.py", "python", content)
        names = [f.name for f in functions]
        # Regex fallback should still find functions
        assert "valid_func" in names
        assert "another_func" in names

    def test_generic_extractor(self):
        # Test with an unsupported language that falls back to generic
        content = '''
def some_func():
    pass

function another():
    return
'''
        functions = extract_functions("test.unknown", "unknown", content)
        names = [f.name for f in functions]
        assert len(names) >= 1  # Generic should find at least one


class TestShouldExclude:
    def test_directory_pattern(self):
        assert should_exclude("src/tests/test_foo.py", ["tests/"])
        assert should_exclude("node_modules/lodash/index.js", ["node_modules/"])
        assert not should_exclude("src/main.py", ["tests/"])

    def test_glob_pattern(self):
        assert should_exclude("test_foo.py", ["test_*"])
        assert should_exclude("foo_test.py", ["*_test.*"])
        assert should_exclude("bundle.min.js", ["*.min.js"])
        assert not should_exclude("main.py", ["test_*"])

    def test_exact_match(self):
        assert should_exclude("path/to/__pycache__/module.pyc", ["__pycache__"])
        assert not should_exclude("src/main.py", ["__pycache__"])

    def test_multiple_patterns(self):
        patterns = ["tests/", "test_*", "*.min.js"]
        assert should_exclude("tests/test_main.py", patterns)
        assert should_exclude("test_utils.py", patterns)
        assert should_exclude("app.min.js", patterns)
        assert not should_exclude("src/utils.py", patterns)


class TestChecklistBuilder:
    def test_build_from_directory(self, tmp_path):
        # Create test files
        src_dir = tmp_path / "src"
        src_dir.mkdir()
        (src_dir / "foo.py").write_text("def foo(): pass\ndef bar(): pass\n")
        (src_dir / "baz.js").write_text("function baz() {}\n")
        out_dir = tmp_path / "out"

        checklist = build_checklist(str(src_dir), str(out_dir), exclude_patterns=[])

        assert checklist["total_files"] == 2
        assert checklist["total_functions"] == 3
        assert "generated_at" in checklist
        assert (out_dir / "checklist.json").exists()

    def test_excludes_test_files(self, tmp_path):
        src_dir = tmp_path / "src"
        src_dir.mkdir()
        (src_dir / "main.py").write_text("def main(): pass\n")
        (src_dir / "test_main.py").write_text("def test_main(): pass\n")
        out_dir = tmp_path / "out"

        checklist = build_checklist(str(src_dir), str(out_dir))  # Uses DEFAULT_EXCLUDES

        assert checklist["total_files"] == 1
        assert checklist["total_functions"] == 1

    def test_single_file(self, tmp_path):
        test_file = tmp_path / "single.py"
        test_file.write_text("def one(): pass\ndef two(): pass\n")
        out_dir = tmp_path / "out"

        checklist = build_checklist(str(test_file), str(out_dir), exclude_patterns=[])

        assert checklist["total_files"] == 1
        assert checklist["total_functions"] == 2

    def test_multiple_languages(self, tmp_path):
        src_dir = tmp_path / "src"
        src_dir.mkdir()
        (src_dir / "app.py").write_text("def py_func(): pass\n")
        (src_dir / "app.js").write_text("function js_func() {}\n")
        (src_dir / "app.go").write_text("func go_func() {}\n")
        out_dir = tmp_path / "out"

        checklist = build_checklist(str(src_dir), str(out_dir), exclude_patterns=[])

        assert checklist["total_files"] == 3
        assert checklist["total_functions"] == 3


class TestCoverageTracking:
    def test_update_checklist_coverage(self):
        checklist = {
            "files": [
                {
                    "path": "app.py",
                    "functions": [
                        {"name": "foo", "checked": False},
                        {"name": "bar", "checked": False},
                    ]
                }
            ]
        }

        checked = [
            {"file": "app.py", "function": "foo"}
        ]

        updated = update_checklist_coverage(checklist, checked)

        assert updated["files"][0]["functions"][0]["checked"] is True
        assert updated["files"][0]["functions"][1]["checked"] is False

    def test_get_coverage_stats(self):
        checklist = {
            "files": [
                {
                    "path": "app.py",
                    "functions": [
                        {"name": "foo", "checked": True},
                        {"name": "bar", "checked": False},
                        {"name": "baz", "checked": True},
                    ]
                }
            ]
        }

        stats = get_coverage_stats(checklist)

        assert stats["total_functions"] == 3
        assert stats["checked_functions"] == 2
        assert stats["coverage_percent"] == pytest.approx(66.67, rel=0.1)

    def test_get_coverage_stats_empty(self):
        checklist = {"files": []}
        stats = get_coverage_stats(checklist)

        assert stats["total_functions"] == 0
        assert stats["checked_functions"] == 0
        assert stats["coverage_percent"] == 0


class TestSchemaValidation:
    def test_valid_checklist(self):
        checklist = create_empty_checklist("/tmp/test")
        valid, errors = validate_checklist(checklist)
        assert valid
        assert len(errors) == 0

    def test_invalid_checklist_missing_field(self):
        checklist = {"files": []}  # Missing required fields
        valid, errors = validate_checklist(checklist)
        assert not valid
        assert len(errors) > 0

    def test_valid_findings(self):
        findings = create_empty_findings("A", "/tmp/test", "sql_injection")
        valid, errors = validate_findings(findings)
        assert valid
        assert len(errors) == 0

    def test_invalid_findings_missing_stage(self):
        findings = {"findings": []}  # Missing 'stage'
        valid, errors = validate_findings(findings)
        assert not valid

    def test_finding_creation(self):
        finding = create_finding(
            finding_id="FIND-001",
            file="foo.py",
            function="vulnerable",
            line=42,
            vuln_type="command_injection",
            status="poc_success"
        )
        assert finding["id"] == "FIND-001"
        assert finding["file"] == "foo.py"
        assert finding["status"] == "poc_success"

    def test_finding_default_status(self):
        finding = create_finding(
            finding_id="FIND-002",
            file="bar.py",
            function="test",
            line=10,
            vuln_type="xss"
        )
        assert finding["status"] == "not_disproven"  # Default

    def test_validate_attack_tree(self):
        from packages.exploitability_validation import validate_attack_tree
        tree = {
            "root": "Exploit vulnerability",
            "updated_at": "2026-01-22T10:00:00",
            "nodes": []
        }
        valid, errors = validate_attack_tree(tree)
        assert valid

    def test_validate_attack_tree_invalid(self):
        from packages.exploitability_validation import validate_attack_tree
        tree = {"nodes": []}  # Missing 'root'
        valid, errors = validate_attack_tree(tree)
        assert not valid

    def test_validate_attack_paths(self):
        from packages.exploitability_validation import validate_attack_paths
        paths = [
            {
                "id": "PATH-001",
                "path": [
                    {"step": 1, "action": "inject payload", "result": "reflected"}
                ],
                "proximity": 3
            }
        ]
        valid, errors = validate_attack_paths(paths)
        assert valid

    def test_validate_attack_paths_empty(self):
        from packages.exploitability_validation import validate_attack_paths
        valid, errors = validate_attack_paths([])
        assert valid  # Empty is valid

    def test_validate_attack_surface(self):
        from packages.exploitability_validation import validate_attack_surface
        surface = {
            "sources": [{"type": "http_param", "location": "request.args.id"}],
            "sinks": [{"type": "command_exec", "location": "os.system()"}],
            "trust_boundaries": []
        }
        valid, errors = validate_attack_surface(surface)
        assert valid

    def test_validate_attack_surface_invalid(self):
        from packages.exploitability_validation import validate_attack_surface
        surface = {"sources": "not_a_list"}  # Wrong type
        valid, errors = validate_attack_surface(surface)
        assert not valid


class TestStageEnum:
    def test_stage_values(self):
        assert Stage.INVENTORY.value == "0"
        assert Stage.ONESHOT.value == "A"
        assert Stage.PROCESS.value == "B"
        assert Stage.SANITY.value == "C"
        assert Stage.RULING.value == "D"
        assert Stage.FEASIBILITY.value == "E"

    def test_stage_iteration(self):
        stages = list(Stage)
        assert len(stages) == 6


class TestStageStatus:
    def test_status_values(self):
        assert StageStatus.PENDING.value == "pending"
        assert StageStatus.RUNNING.value == "running"
        assert StageStatus.COMPLETED.value == "completed"
        assert StageStatus.FAILED.value == "failed"
        assert StageStatus.SKIPPED.value == "skipped"


class TestPipelineConfig:
    def test_defaults(self):
        config = PipelineConfig(
            target_path="/tmp/test",
            workdir="/tmp/out"
        )
        assert config.vuln_type is None
        assert config.binary_path is None
        assert config.skip_feasibility is False
        assert config.max_retries == 3

    def test_custom_values(self):
        config = PipelineConfig(
            target_path="/code",
            workdir="/out",
            vuln_type="sql_injection",
            binary_path="/bin/app",
            skip_feasibility=True,
            max_retries=5
        )
        assert config.vuln_type == "sql_injection"
        assert config.binary_path == "/bin/app"
        assert config.skip_feasibility is True
        assert config.max_retries == 5


class TestPipelineState:
    def test_get_output_path(self, tmp_path):
        config = PipelineConfig(target_path="/code", workdir=str(tmp_path))
        state = PipelineState(config=config)

        path = state.get_output_path("test.json")
        assert path == str(tmp_path / "test.json")

    def test_save_and_load_json(self, tmp_path):
        config = PipelineConfig(target_path="/code", workdir=str(tmp_path))
        state = PipelineState(config=config)

        data = {"key": "value", "number": 42}
        path = state.save_json("data.json", data)

        assert Path(path).exists()

        loaded = state.load_json("data.json")
        assert loaded == data

    def test_load_nonexistent_json(self, tmp_path):
        config = PipelineConfig(target_path="/code", workdir=str(tmp_path))
        state = PipelineState(config=config)

        result = state.load_json("nonexistent.json")
        assert result is None


class TestValidationOrchestrator:
    def test_stage_0_inventory(self, tmp_path):
        # Create test file
        (tmp_path / "app.py").write_text("def vulnerable(): pass\n")

        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(
            target_path=str(tmp_path),
            workdir=str(workdir)
        )

        orchestrator = ValidationOrchestrator(config)
        result = orchestrator._execute_stage(Stage.INVENTORY)

        assert result.status == StageStatus.COMPLETED
        assert orchestrator.state.checklist is not None
        assert orchestrator.state.checklist["total_functions"] == 1

        # Check file was saved
        checklist_file = workdir / "checklist.json"
        assert checklist_file.exists()

    def test_stage_a_oneshot(self, tmp_path):
        (tmp_path / "app.py").write_text("def vuln(): pass\n")
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        # Run stage 0 first
        orchestrator._execute_stage(Stage.INVENTORY)
        # Then stage A
        result = orchestrator._execute_stage(Stage.ONESHOT)

        assert result.status == StageStatus.COMPLETED
        assert orchestrator.state.findings is not None
        assert "findings" in orchestrator.state.findings

    def test_stage_b_process_skipped_no_findings(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        # Set up findings with no "not_disproven"
        orchestrator.state.findings = {"findings": []}

        result = orchestrator._execute_stage(Stage.PROCESS)
        assert result.status == StageStatus.COMPLETED
        assert result.output_files == []  # Skipped

    def test_stage_c_sanity(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "A",
            "findings": [{"id": "F1", "status": "poc_success"}]
        }

        result = orchestrator._execute_stage(Stage.SANITY)
        assert result.status == StageStatus.COMPLETED
        assert "_instruction" in orchestrator.state.findings

    def test_stage_d_ruling(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "C",
            "findings": [
                {"id": "F1", "sanity_check": {"passed": True}},
                {"id": "F2", "sanity_check": {"passed": False}},
            ]
        }

        result = orchestrator._execute_stage(Stage.RULING)
        assert result.status == StageStatus.COMPLETED

        # F2 should be ruled out
        f2 = next(f for f in orchestrator.state.findings["findings"] if f["id"] == "F2")
        assert f2["status"] == "ruled_out"

    def test_stage_e_skip_feasibility(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(
            target_path=str(tmp_path),
            workdir=str(workdir),
            skip_feasibility=True
        )
        orchestrator = ValidationOrchestrator(config)
        orchestrator.state.findings = {"findings": []}

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED
        assert result.output_files == []

    def test_stage_e_no_memory_corruption(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        # Web vuln - not memory corruption
        orchestrator.state.findings = {
            "findings": [{
                "id": "F1",
                "vuln_type": "sql_injection",
                "ruling": {"status": "confirmed"}
            }]
        }

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED

        f1 = orchestrator.state.findings["findings"][0]
        assert f1["final_status"] == "confirmed"
        assert f1["feasibility"]["status"] == "not_applicable"

    def test_should_skip_remaining_all_disproven(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "findings": [
                {"status": "disproven"},
                {"status": "disproven"},
            ]
        }

        from packages.exploitability_validation.orchestrator import StageResult
        result = StageResult(stage=Stage.ONESHOT, status=StageStatus.COMPLETED)

        should_skip = orchestrator._should_skip_remaining(Stage.ONESHOT, result)
        assert should_skip is True

    def test_should_skip_remaining_has_findings(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "findings": [
                {"status": "poc_success"},
                {"status": "disproven"},
            ]
        }

        from packages.exploitability_validation.orchestrator import StageResult
        result = StageResult(stage=Stage.ONESHOT, status=StageStatus.COMPLETED)

        should_skip = orchestrator._should_skip_remaining(Stage.ONESHOT, result)
        assert should_skip is False

    def test_generate_report(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        from datetime import datetime
        orchestrator.state.started_at = datetime.now()
        orchestrator.state.completed_at = datetime.now()
        orchestrator.state.checklist = {"total_functions": 10, "files": []}
        orchestrator.state.findings = {
            "findings": [{"id": "F1", "final_status": "confirmed", "vuln_type": "xss",
                         "file": "app.py", "line": 42, "function": "render"}]
        }

        orchestrator._generate_report()

        report_file = workdir / "validation-report.md"
        assert report_file.exists()
        content = report_file.read_text()
        assert "Exploitability Validation Report" in content
        assert "Confirmed" in content  # Human-readable status display


class TestRunValidation:
    def test_full_pipeline(self, tmp_path):
        # Create test file
        (tmp_path / "app.py").write_text('''
def execute_command(user_input):
    import os
    os.system(user_input)  # Command injection!
''')

        workdir = tmp_path / "validation"

        state = run_validation(
            target_path=str(tmp_path),
            vuln_type="command_injection",
            workdir=str(workdir),
            skip_feasibility=True
        )

        assert state.completed_at is not None
        assert state.checklist is not None
        assert state.findings is not None

        # Check report was generated
        report = workdir / "validation-report.md"
        assert report.exists()


class TestSarifConversion:
    """Test SARIF format handling."""

    def test_is_sarif_format_with_schema(self, tmp_path):
        """Test detection of SARIF 2.1.0 format."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)

        sarif_data = {
            "$schema": "https://json.schemastore.org/sarif-2.1.0.json",
            "version": "2.1.0",
            "runs": []
        }
        assert orchestrator._is_sarif_format(sarif_data) is True

    def test_is_sarif_format_v20(self, tmp_path):
        """Test detection of SARIF 2.0 format (no schema)."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)

        sarif_data = {"version": "2.0", "runs": []}
        assert orchestrator._is_sarif_format(sarif_data) is True

    def test_is_sarif_format_not_sarif(self, tmp_path):
        """Test rejection of non-SARIF format."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)

        non_sarif = {"findings": [], "stage": "A"}
        assert orchestrator._is_sarif_format(non_sarif) is False

    def test_convert_sarif_basic(self, tmp_path):
        """Test basic SARIF to findings conversion."""
        sarif_data = {
            "$schema": "https://json.schemastore.org/sarif-2.1.0.json",
            "version": "2.1.0",
            "runs": [{
                "tool": {"driver": {"name": "semgrep"}},
                "results": [{
                    "ruleId": "sql-injection",
                    "message": {"text": "Possible SQL injection"},
                    "locations": [{
                        "physicalLocation": {
                            "artifactLocation": {"uri": "src/db.py"},
                            "region": {"startLine": 42, "snippet": {"text": "cursor.execute(query)"}}
                        }
                    }]
                }]
            }]
        }

        findings = convert_sarif_data(sarif_data, str(tmp_path))

        assert findings["source"] == "sarif"
        assert len(findings["findings"]) == 1
        f = findings["findings"][0]
        assert f["file"] == "src/db.py"
        assert f["line"] == 42
        assert f["vuln_type"] == "sql_injection"  # Normalized to underscores
        assert "SQL injection" in f["message"]
        assert isinstance(f["proof"], dict)
        assert f["proof"]["vulnerable_code"] == "cursor.execute(query)"

    def test_convert_sarif_deduplication(self, tmp_path):
        """Test that SARIF conversion deduplicates by fingerprint."""
        sarif_data = {
            "version": "2.1.0",
            "runs": [{
                "tool": {"driver": {"name": "test"}},
                "results": [
                    {
                        "ruleId": "vuln1",
                        "fingerprints": {"primaryLocationLineHash": "abc123"},
                        "message": {"text": "First"},
                        "locations": [{"physicalLocation": {
                            "artifactLocation": {"uri": "test.py"},
                            "region": {"startLine": 10}
                        }}]
                    },
                    {
                        "ruleId": "vuln1",
                        "fingerprints": {"primaryLocationLineHash": "abc123"},  # Duplicate
                        "message": {"text": "Duplicate"},
                        "locations": [{"physicalLocation": {
                            "artifactLocation": {"uri": "test.py"},
                            "region": {"startLine": 10}
                        }}]
                    }
                ]
            }]
        }

        findings = convert_sarif_data(sarif_data, str(tmp_path))
        assert len(findings["findings"]) == 1

    def test_convert_sarif_malformed_result(self, tmp_path):
        """Test that malformed SARIF results are skipped gracefully."""
        sarif_data = {
            "version": "2.1.0",
            "runs": [{
                "tool": {"driver": {"name": "test"}},
                "results": [
                    {},  # Malformed - no locations
                    {"locations": []},  # Malformed - empty locations
                    {  # Valid
                        "ruleId": "valid",
                        "message": {"text": "Valid finding"},
                        "locations": [{"physicalLocation": {
                            "artifactLocation": {"uri": "valid.py"},
                            "region": {"startLine": 5}
                        }}]
                    }
                ]
            }]
        }

        findings = convert_sarif_data(sarif_data, str(tmp_path))
        assert len(findings["findings"]) == 1
        assert findings["findings"][0]["file"] == "valid.py"

    def test_load_findings_sarif_file(self, tmp_path):
        """Test loading findings from SARIF file."""
        sarif_file = tmp_path / "results.sarif"
        sarif_data = {
            "$schema": "https://json.schemastore.org/sarif-2.1.0.json",
            "version": "2.1.0",
            "runs": [{
                "tool": {"driver": {"name": "semgrep"}},
                "results": [{
                    "ruleId": "test-rule",
                    "message": {"text": "Test vuln"},
                    "locations": [{"physicalLocation": {
                        "artifactLocation": {"uri": "app.py"},
                        "region": {"startLine": 100}
                    }}]
                }]
            }]
        }
        sarif_file.write_text(json.dumps(sarif_data))

        workdir = tmp_path / "work"
        workdir.mkdir()

        config = PipelineConfig(
            target_path=str(tmp_path),
            workdir=str(workdir),
            findings_file=str(sarif_file)
        )
        orchestrator = ValidationOrchestrator(config)
        orchestrator._load_existing_findings()

        assert orchestrator.state.findings is not None
        assert len(orchestrator.state.findings["findings"]) == 1
        assert orchestrator.state.checklist is not None
        assert len(orchestrator.state.checklist["files"]) == 1

    def test_deduplicate_findings(self, tmp_path):
        """Test deduplication of findings by file:line:type."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "findings": [
                {"file": "a.py", "line": 10, "vuln_type": "sqli"},
                {"file": "a.py", "line": 10, "vuln_type": "sqli"},  # Dup
                {"file": "a.py", "line": 10, "vuln_type": "xss"},   # Different type
                {"file": "a.py", "line": 20, "vuln_type": "sqli"},  # Different line
            ]
        }

        orchestrator._deduplicate_findings()

        assert len(orchestrator.state.findings["findings"]) == 3


class TestStageBFeasibility:
    """Phase 2: Stage B initializes feasibility on not_disproven findings."""

    def test_stage_b_initializes_feasibility(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        # Set up findings with not_disproven status
        orchestrator.state.findings = {
            "stage": "A",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "FIND-0001", "file": "a.py", "function": "foo", "line": 10,
                 "vuln_type": "sql_injection", "status": "not_disproven"},
            ]
        }

        # Save attack paths and hypotheses that match
        orchestrator.state.save_json("attack-paths.json", [
            {"id": "PATH-001", "finding_id": "FIND-0001", "path": [],
             "proximity": 3, "blockers": ["WAF blocks single quotes"]}
        ])
        orchestrator.state.save_json("hypotheses.json", [
            {"id": "H-001", "finding_id": "FIND-0001", "hypothesis": "Double encoding bypasses WAF",
             "status": "confirmed"}
        ])

        result = orchestrator._execute_stage(Stage.PROCESS)
        assert result.status == StageStatus.COMPLETED

        finding = orchestrator.state.findings["findings"][0]
        assert finding.get("feasibility") is not None
        assert finding["feasibility"]["status"] == "pending"
        assert finding["feasibility"]["chain_breaks"] == ["WAF blocks single quotes"]
        assert finding["feasibility"]["what_would_help"] == ["Double encoding bypasses WAF"]
        assert finding["feasibility"]["attack_path_ref"] == "attack-paths.json#PATH-001"

    def test_stage_b_skips_disproven_findings(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "A",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "FIND-0001", "file": "a.py", "function": "foo", "line": 10,
                 "vuln_type": "sql_injection", "status": "disproven"},
                {"id": "FIND-0002", "file": "b.py", "function": "bar", "line": 20,
                 "vuln_type": "xss", "status": "not_disproven"},
            ]
        }

        result = orchestrator._execute_stage(Stage.PROCESS)
        assert result.status == StageStatus.COMPLETED

        disproven_finding = orchestrator.state.findings["findings"][0]
        active_finding = orchestrator.state.findings["findings"][1]

        # Disproven finding should NOT get feasibility
        assert disproven_finding.get("feasibility") is None
        # Active finding should get feasibility
        assert active_finding.get("feasibility") is not None
        assert active_finding["feasibility"]["status"] == "pending"


    def test_stage_b_preserves_completed_feasibility(self, tmp_path):
        """Pre-existing finding with feasibility.status='analyzed' should not be overwritten by Stage B."""
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "A",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "FIND-0001", "file": "a.py", "function": "foo", "line": 10,
                 "vuln_type": "sql_injection", "status": "not_disproven",
                 "feasibility": {
                     "status": "analyzed",
                     "verdict": "difficult",
                     "chain_breaks": ["WAF"],
                     "what_would_help": ["encoding bypass"],
                 }},
            ]
        }

        result = orchestrator._execute_stage(Stage.PROCESS)
        assert result.status == StageStatus.COMPLETED

        finding = orchestrator.state.findings["findings"][0]
        # Feasibility should NOT be overwritten — status should remain "analyzed"
        assert finding["feasibility"]["status"] == "analyzed"
        assert finding["feasibility"]["verdict"] == "difficult"
        assert finding["feasibility"]["chain_breaks"] == ["WAF"]


class TestStageEMerge:
    """Phase 3: Stage E merges existing feasibility rather than overwriting."""

    def test_stage_e_preserves_feasibility_on_no_binary(self, tmp_path):
        """Web vuln with existing feasibility from Stage B, no binary → data preserved."""
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "D",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "sql_injection",
                "ruling": {"status": "confirmed"},
                "feasibility": {
                    "status": "pending",
                    "chain_breaks": ["WAF blocks single quotes"],
                    "what_would_help": ["Double encoding bypass"],
                    "attack_path_ref": "attack-paths.json#PATH-001",
                }
            }]
        }

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED

        f = orchestrator.state.findings["findings"][0]
        # Verdict "difficult" now maps to confirmed_constrained (not generic "confirmed")
        assert f["final_status"] == "confirmed_constrained"
        # Source analysis should derive a verdict
        assert f["feasibility"]["status"] == "analyzed"
        assert f["feasibility"]["verdict"] == "difficult"  # has both breaks and helps
        assert f["feasibility"]["chain_breaks"] == ["WAF blocks single quotes"]
        assert f["feasibility"]["what_would_help"] == ["Double encoding bypass"]
        assert f["feasibility"]["attack_path_ref"] == "attack-paths.json#PATH-001"

    def test_stage_e_passthrough_web_vuln_with_source_analysis(self, tmp_path):
        """SQL injection with source feasibility → verdict derived, attack_path_ref preserved."""
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "D",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "xss",
                "ruling": {"status": "confirmed"},
                "feasibility": {
                    "status": "pending",
                    "chain_breaks": [],
                    "what_would_help": ["Reflected input in response"],
                    "attack_path_ref": "attack-paths.json#PATH-002",
                }
            }]
        }

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED

        f = orchestrator.state.findings["findings"][0]
        # No chain_breaks but has what_would_help → "difficult" (unknowns remain)
        assert f["feasibility"]["verdict"] == "difficult"
        assert f["feasibility"]["attack_path_ref"] == "attack-paths.json#PATH-002"

    def test_derive_verdict_from_source(self, tmp_path):
        """Unit test the _derive_verdict_from_source helper."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)

        # Both breaks and helps → difficult
        assert orchestrator._derive_verdict_from_source(
            {"chain_breaks": ["blocker"], "what_would_help": ["suggestion"]}
        ) == "difficult"

        # Breaks but no helps → unlikely
        assert orchestrator._derive_verdict_from_source(
            {"chain_breaks": ["blocker"], "what_would_help": []}
        ) == "unlikely"

        # No breaks but has what_would_help → difficult (unknowns remain)
        assert orchestrator._derive_verdict_from_source(
            {"chain_breaks": [], "what_would_help": ["suggestion"]}
        ) == "difficult"

        # No breaks and no what_would_help → likely_exploitable
        assert orchestrator._derive_verdict_from_source(
            {"chain_breaks": [], "what_would_help": []}
        ) == "likely_exploitable"

        # Empty → likely_exploitable (defaults to empty lists)
        assert orchestrator._derive_verdict_from_source({}) == "likely_exploitable"

    def test_stage_e_pending_feasibility_with_empty_lists(self, tmp_path):
        """Stage B ran but found no blockers → should still derive verdict, not 'not_applicable'."""
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "D",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "sql_injection",
                "ruling": {"status": "confirmed"},
                "feasibility": {
                    "status": "pending",
                    "chain_breaks": [],
                    "what_would_help": [],
                    "attack_path_ref": None,
                }
            }]
        }

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED

        f = orchestrator.state.findings["findings"][0]
        # Stage B ran (status=pending), so should derive verdict even with empty lists
        assert f["feasibility"]["status"] == "analyzed"
        assert f["feasibility"]["verdict"] == "likely_exploitable"

    def test_stage_e_no_feasibility_stays_not_applicable(self, tmp_path):
        """Web vuln without existing feasibility → status stays not_applicable."""
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "D",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "sql_injection",
                "ruling": {"status": "confirmed"},
            }]
        }

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED

        f = orchestrator.state.findings["findings"][0]
        assert f["feasibility"]["status"] == "not_applicable"
        assert f["final_status"] == "confirmed"


class TestNormalizeStatus:
    """Test normalize_status handles all legacy formats."""

    def test_already_canonical(self):
        from packages.exploitability_validation.orchestrator import normalize_status
        assert normalize_status("exploitable") == "exploitable"
        assert normalize_status("confirmed") == "confirmed"
        assert normalize_status("ruled_out") == "ruled_out"
        assert normalize_status("likely_exploitable") == "likely_exploitable"

    def test_all_caps_legacy(self):
        from packages.exploitability_validation.orchestrator import normalize_status
        assert normalize_status("EXPLOITABLE") == "exploitable"
        assert normalize_status("CONFIRMED") == "confirmed"
        assert normalize_status("CONFIRMED_CONSTRAINED") == "confirmed_constrained"
        assert normalize_status("CONFIRMED_BLOCKED") == "confirmed_blocked"
        assert normalize_status("CONFIRMED_UNVERIFIED") == "confirmed_unverified"
        assert normalize_status("RULED_OUT") == "ruled_out"

    def test_title_case_verdicts(self):
        from packages.exploitability_validation.orchestrator import normalize_status
        assert normalize_status("Exploitable") == "exploitable"
        assert normalize_status("Likely exploitable") == "likely_exploitable"
        assert normalize_status("Difficult") == "difficult"
        assert normalize_status("Unlikely") == "unlikely"
        assert normalize_status("Unknown") == "unknown"

    def test_fallback_lowercases(self):
        from packages.exploitability_validation.orchestrator import normalize_status
        assert normalize_status("SOME NEW STATUS") == "some_new_status"
        assert normalize_status("Mixed Case Thing") == "mixed_case_thing"

    def test_empty_passthrough(self):
        from packages.exploitability_validation.orchestrator import normalize_status
        assert normalize_status("") == ""
        assert normalize_status(None) is None

    def test_whitespace_stripped(self):
        from packages.exploitability_validation.orchestrator import normalize_status
        assert normalize_status("  exploitable  ") == "exploitable"
        assert normalize_status("\texploitable\n") == "exploitable"
        assert normalize_status("  CONFIRMED  ") == "confirmed"
        assert normalize_status("   ") is None

    def test_non_string_coerced(self):
        from packages.exploitability_validation.orchestrator import normalize_status
        # Non-string inputs should not crash
        assert normalize_status(True) is not None
        assert normalize_status(123) is not None
        assert normalize_status(False) is False  # falsy, returned early

    def test_normalize_findings_skips_non_dict(self):
        from packages.exploitability_validation.orchestrator import normalize_findings
        data = {
            "findings": [
                {"id": "FIND-0001", "status": "CONFIRMED"},
                None,
                "garbage",
                {"id": "FIND-0002", "status": "RULED_OUT"},
            ]
        }
        normalize_findings(data)
        assert data["findings"][0]["status"] == "confirmed"
        assert data["findings"][1] is None  # untouched
        assert data["findings"][2] == "garbage"  # untouched
        assert data["findings"][3]["status"] == "ruled_out"

    def test_normalize_findings_on_load(self, tmp_path):
        """Verify that loading findings.json normalizes status fields including feasibility.status."""
        config = PipelineConfig(target_path="/code", workdir=str(tmp_path))
        state = PipelineState(config=config)

        # Write findings with ALL_CAPS (simulating LLM output)
        import json
        findings = {
            "stage": "D", "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001", "file": "a.py", "function": "foo",
                "line": 10, "vuln_type": "sql_injection",
                "status": "confirmed",
                "ruling": {"status": "CONFIRMED"},
                "final_status": "EXPLOITABLE",
                "feasibility": {"verdict": "Likely exploitable", "status": "ANALYZED"}
            }]
        }
        with open(tmp_path / "findings.json", 'w') as f:
            json.dump(findings, f)

        loaded = state.load_json("findings.json")
        f = loaded["findings"][0]
        assert f["ruling"]["status"] == "confirmed"
        assert f["final_status"] == "exploitable"
        assert f["feasibility"]["verdict"] == "likely_exploitable"
        assert f["feasibility"]["status"] == "analyzed"

    def test_save_json_normalizes_findings(self, tmp_path):
        """Verify that save_json normalizes findings before writing to disk (Issue 9)."""
        import json
        config = PipelineConfig(target_path="/code", workdir=str(tmp_path))
        state = PipelineState(config=config)

        findings = {
            "stage": "D", "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001", "file": "a.py", "function": "foo",
                "line": 10, "vuln_type": "sql_injection",
                "status": "CONFIRMED",
                "final_status": "EXPLOITABLE",
                "ruling": {"status": "CONFIRMED"},
                "feasibility": {"verdict": "Likely exploitable", "status": "ANALYZED"}
            }]
        }
        state.save_json("findings.json", findings)

        # Read directly from disk (bypass load_json normalization)
        with open(tmp_path / "findings.json", 'r') as f:
            raw = json.load(f)

        f = raw["findings"][0]
        assert f["status"] == "confirmed"
        assert f["final_status"] == "exploitable"
        assert f["ruling"]["status"] == "confirmed"
        assert f["feasibility"]["verdict"] == "likely_exploitable"
        assert f["feasibility"]["status"] == "analyzed"


class TestFinalizePreservesAnalyzedVerdict:
    """Verify _finalize_non_memory_findings preserves already-analyzed feasibility."""

    def test_finalize_preserves_analyzed_verdict(self, tmp_path):
        """Finding with feasibility already analyzed + verdict should not be overwritten."""
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "D",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "sql_injection",
                "ruling": {"status": "confirmed"},
                "feasibility": {
                    "status": "analyzed",
                    "verdict": "likely_exploitable",
                }
            }]
        }

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED

        f = orchestrator.state.findings["findings"][0]
        assert f["feasibility"]["status"] == "analyzed"
        assert f["feasibility"]["verdict"] == "likely_exploitable"
        assert f["final_status"] == "likely_exploitable"  # preserves confidence distinction


class TestStageEWithNoRuling:
    """Verify Stage E handles findings with ruling=None."""

    def test_stage_e_finding_with_no_ruling(self, tmp_path):
        """Finding with ruling=None should not crash Stage E (just not be in confirmed list)."""
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        orchestrator.state.findings = {
            "stage": "D",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "sql_injection",
                "ruling": None,
            }]
        }

        result = orchestrator._execute_stage(Stage.FEASIBILITY)
        assert result.status == StageStatus.COMPLETED

        f = orchestrator.state.findings["findings"][0]
        # No ruling → not confirmed → no final_status set by Stage E
        assert f.get("final_status") is None


class TestNormalizeAllNestedFields:
    """Verify normalize_findings reaches all nested status fields."""

    def test_normalize_findings_all_nested_fields(self):
        from packages.exploitability_validation.orchestrator import normalize_findings

        data = {
            "stage": "E",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "file": "x.py",
                "line": 1,
                "vuln_type": "xss",
                "status": "CONFIRMED",
                "final_status": "EXPLOITABLE",
                "ruling": {"status": "CONFIRMED"},
                "feasibility": {
                    "status": "ANALYZED",
                    "verdict": "LIKELY_EXPLOITABLE",
                },
            }]
        }
        normalize_findings(data)
        f = data["findings"][0]
        assert f["status"] == "confirmed"
        assert f["final_status"] == "exploitable"
        assert f["ruling"]["status"] == "confirmed"
        assert f["feasibility"]["status"] == "analyzed"
        assert f["feasibility"]["verdict"] == "likely_exploitable"


class TestStatusDisplay:
    """Verify all status enum values have entries in STATUS_DISPLAY."""

    def test_all_final_status_values_have_display(self):
        from packages.exploitability_validation.orchestrator import STATUS_DISPLAY
        from packages.exploitability_validation.schemas import FINDING_SCHEMA

        final_status_enum = FINDING_SCHEMA["properties"]["final_status"]["enum"]
        for status in final_status_enum:
            assert status in STATUS_DISPLAY, f"final_status '{status}' missing from STATUS_DISPLAY"

    def test_all_verdict_values_have_display(self):
        from packages.exploitability_validation.orchestrator import STATUS_DISPLAY
        from packages.exploitability_validation.schemas import FINDING_SCHEMA

        verdict_enum = FINDING_SCHEMA["properties"]["feasibility"]["properties"]["verdict"]["enum"]
        for verdict in verdict_enum:
            assert verdict in STATUS_DISPLAY, f"verdict '{verdict}' missing from STATUS_DISPLAY"

    def test_all_ruling_status_values_have_display(self):
        from packages.exploitability_validation.orchestrator import STATUS_DISPLAY
        from packages.exploitability_validation.schemas import FINDING_SCHEMA

        ruling_status_enum = FINDING_SCHEMA["properties"]["ruling"]["properties"]["status"]["enum"]
        for status in ruling_status_enum:
            assert status in STATUS_DISPLAY, f"ruling.status '{status}' missing from STATUS_DISPLAY"


class TestNormalizeRuleId:
    """Test normalize_rule_id validates against schema enum."""

    def test_cwe_mapping(self):
        from packages.exploitability_validation.orchestrator import normalize_rule_id
        assert normalize_rule_id("CWE-89", "semgrep") == "sql_injection"
        assert normalize_rule_id("CWE-79", "codeql") == "xss"
        assert normalize_rule_id("CWE-416", "codeql") == "use_after_free"

    def test_keyword_matching(self):
        from packages.exploitability_validation.orchestrator import normalize_rule_id
        assert normalize_rule_id("java/sql-injection", "codeql") == "sql_injection"
        assert normalize_rule_id("security.xss.reflected", "semgrep") == "xss"
        assert normalize_rule_id("engine.semgrep.rules.crypto.raptor.crypto.weak-hash.python", "semgrep") == "weak_crypto"

    def test_exact_enum_match(self):
        from packages.exploitability_validation.orchestrator import normalize_rule_id
        assert normalize_rule_id("deserialization", "tool") == "deserialization"
        assert normalize_rule_id("ssrf", "tool") == "ssrf"

    def test_fallback_to_other(self):
        """Unrecognized rule IDs must return 'other', not arbitrary strings."""
        from packages.exploitability_validation.orchestrator import normalize_rule_id
        assert normalize_rule_id("some-custom-rule", "tool") == "other"
        assert normalize_rule_id("engine.semgrep.rules.python.misc.debug-print", "semgrep") == "other"
        assert normalize_rule_id("", "tool") == "other"

    def test_never_returns_non_enum_value(self):
        """No input should produce a value outside the schema enum."""
        from packages.exploitability_validation.orchestrator import normalize_rule_id
        valid = {
            'command_injection', 'sql_injection', 'xss', 'path_traversal',
            'ssrf', 'deserialization', 'buffer_overflow', 'heap_overflow',
            'stack_overflow', 'format_string', 'use_after_free', 'double_free',
            'integer_overflow', 'out_of_bounds_read', 'out_of_bounds_write',
            'hardcoded_secret', 'weak_crypto', 'other',
        }
        test_inputs = [
            "CWE-89", "java/sql-injection", "unknown-rule", "",
            "engine.semgrep.rules.python.debug", "custom.thing.foo-bar",
            "rules.security.misc.logging", "codeql.java.ql.custom-check",
        ]
        for rule_id in test_inputs:
            result = normalize_rule_id(rule_id, "test")
            assert result in valid, f"normalize_rule_id({rule_id!r}) returned {result!r} which is not in schema enum"


class TestMinimalChecklist:
    """Test _create_minimal_checklist produces schema-valid output."""

    def test_minimal_checklist_has_language_field(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()

        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        orchestrator.state.findings = {
            "stage": "A",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "FIND-0001", "file": "app.py", "line": 1,
                 "vuln_type": "xss", "status": "not_disproven", "function": "f"},
            ]
        }

        orchestrator._create_minimal_checklist()
        checklist = orchestrator.state.checklist
        assert len(checklist["files"]) == 1
        assert checklist["files"][0]["language"] == "unknown"
        assert checklist["files"][0]["path"] == "app.py"

        # Must pass schema validation
        from packages.exploitability_validation import validate_checklist
        is_valid, errors = validate_checklist(checklist)
        assert is_valid, f"Minimal checklist failed validation: {errors}"


class TestMissingFindings:
    """Test stages raise clear errors when findings.json is missing."""

    def test_stage_b_raises_on_missing_findings(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        # Don't set findings, don't create findings.json
        with pytest.raises(ValueError, match="Stage B.*No findings"):
            orchestrator._run_stage_b()

    def test_stage_c_raises_on_missing_findings(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        with pytest.raises(ValueError, match="Stage C.*No findings"):
            orchestrator._run_stage_c()

    def test_stage_d_raises_on_missing_findings(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        with pytest.raises(ValueError, match="Stage D.*No findings"):
            orchestrator._run_stage_d()

    def test_stage_e_raises_on_missing_findings(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        with pytest.raises(ValueError, match="Stage E.*No findings"):
            orchestrator._run_stage_e()


class TestReportIncludesAllConfirmed:
    """Test _generate_report includes all final_status values."""

    def test_report_includes_confirmed_unverified(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        orchestrator.state.findings = {
            "stage": "E",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "file": "app.c",
                "function": "vuln",
                "line": 10,
                "vuln_type": "buffer_overflow",
                "status": "confirmed",
                "final_status": "confirmed_unverified",
            }]
        }
        orchestrator._generate_report()
        report_path = orchestrator.state.get_output_path("validation-report.md")
        report = Path(report_path).read_text()
        assert "FIND-0001" in report
        assert "buffer_overflow" in report


class TestShouldExcludeSegmentMatching:
    """Test should_exclude uses path segment matching, not substring."""

    def test_no_false_positive_on_substring(self):
        """Pattern 'test' should NOT match 'attestation.py' via substring."""
        assert not should_exclude("attestation.py", ["test"])
        assert not should_exclude("src/protest.py", ["test"])
        assert not should_exclude("contest/main.py", ["test"])

    def test_exact_segment_match(self):
        """Pattern 'test' SHOULD match if it's a full path segment or filename."""
        assert should_exclude("test", ["test"])
        assert should_exclude("src/test/main.py", ["test"])

    def test_directory_patterns_still_work(self):
        """Directory patterns (ending with /) still match path segments."""
        assert should_exclude("src/tests/foo.py", ["tests/"])
        assert not should_exclude("src/contests/foo.py", ["tests/"])


class TestDedupFilename:
    """Verify dedup-only path uses the canonical findings.json filename."""

    def test_dedup_saves_as_findings_json(self, tmp_path):
        """Deduplication path should save to 'findings.json' not 'deduplicated_findings.json'."""
        from packages.exploitability_validation.agentic import run_validation_phase

        # Create a minimal SARIF file
        sarif = {
            "version": "2.1.0",
            "$schema": "https://json.schemastore.org/sarif-2.1.0.json",
            "runs": [{
                "tool": {"driver": {"name": "test-tool", "rules": []}},
                "results": [{
                    "ruleId": "test/rule-1",
                    "message": {"text": "Test finding"},
                    "locations": [{
                        "physicalLocation": {
                            "artifactLocation": {"uri": "app.py"},
                            "region": {"startLine": 10}
                        }
                    }],
                    "level": "error"
                }]
            }]
        }
        sarif_file = tmp_path / "results.sarif"
        with open(sarif_file, 'w') as f:
            json.dump(sarif, f)

        out_dir = tmp_path / "out"
        out_dir.mkdir()

        result, count = run_validation_phase(
            repo_path=str(tmp_path),
            out_dir=out_dir,
            sarif_files=[sarif_file],
            total_findings=1,
            llm_available=False,
            binary_path=None,
        )

        # Should save to findings.json, not deduplicated_findings.json
        assert (out_dir / "validation" / "findings.json").exists()
        assert not (out_dir / "validation" / "deduplicated_findings.json").exists()


class TestValidatedFindingsNotShadowed:
    """Verify validated_findings stays an integer through the agentic pipeline."""

    def test_run_validation_phase_returns_int(self, tmp_path):
        """run_validation_phase always returns (dict, int)."""
        from packages.exploitability_validation.agentic import run_validation_phase

        result, count = run_validation_phase(
            repo_path=str(tmp_path),
            out_dir=tmp_path,
            sarif_files=[],
            total_findings=0,
            skip_validation=True,
        )
        assert isinstance(count, int)
        assert isinstance(result, dict)

    def test_skip_validation_returns_total(self, tmp_path):
        """When skipped, validated_findings equals total_findings."""
        from packages.exploitability_validation.agentic import run_validation_phase

        result, count = run_validation_phase(
            repo_path=str(tmp_path),
            out_dir=tmp_path,
            sarif_files=[],
            total_findings=42,
            skip_validation=True,
        )
        assert count == 42


class TestStageDSetsFinalStatus:
    """Stage D must set final_status so the report works even when Stage E is skipped."""

    def test_confirmed_finding_gets_final_status(self, tmp_path):
        """Confirmed findings get final_status='confirmed' in Stage D."""
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orch = ValidationOrchestrator(config)
        orch.state.findings = {
            "stage": "C", "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "F-001", "file": "a.py", "line": 1, "function": "f",
                 "vuln_type": "xss", "status": "not_disproven",
                 "sanity_check": {"passed": True}}
            ]
        }
        orch._run_stage_d()
        f = orch.state.findings["findings"][0]
        assert f["final_status"] == "confirmed"

    def test_ruled_out_finding_gets_final_status(self, tmp_path):
        """Ruled-out findings get final_status='ruled_out' in Stage D."""
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orch = ValidationOrchestrator(config)
        orch.state.findings = {
            "stage": "C", "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "F-001", "file": "a.py", "line": 1, "function": "f",
                 "vuln_type": "xss", "status": "not_disproven",
                 "sanity_check": {"passed": False}}
            ]
        }
        orch._run_stage_d()
        f = orch.state.findings["findings"][0]
        assert f["final_status"] == "ruled_out"

    def test_no_sanity_check_gets_final_status(self, tmp_path):
        """Findings without sanity check get final_status='confirmed' in Stage D."""
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orch = ValidationOrchestrator(config)
        orch.state.findings = {
            "stage": "C", "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "F-001", "file": "a.py", "line": 1, "function": "f",
                 "vuln_type": "xss", "status": "not_disproven"}
            ]
        }
        orch._run_stage_d()
        f = orch.state.findings["findings"][0]
        assert f["final_status"] == "confirmed"


class TestLoadJsonCorruptFile:
    """load_json should handle corrupt/empty JSON files gracefully."""

    def _make_state(self, tmp_path):
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        return PipelineState(config=config)

    def test_empty_file_returns_none(self, tmp_path):
        """Empty JSON file returns None instead of crashing."""
        state = self._make_state(tmp_path)
        empty_file = tmp_path / "empty.json"
        empty_file.write_text("")
        result = state.load_json("empty.json")
        assert result is None

    def test_malformed_json_returns_none(self, tmp_path):
        """Malformed JSON returns None instead of crashing."""
        state = self._make_state(tmp_path)
        bad_file = tmp_path / "bad.json"
        bad_file.write_text("{invalid json content")
        result = state.load_json("bad.json")
        assert result is None

    def test_valid_json_still_works(self, tmp_path):
        """Valid JSON files still load correctly."""
        state = self._make_state(tmp_path)
        good_file = tmp_path / "good.json"
        good_file.write_text('{"key": "value"}')
        result = state.load_json("good.json")
        assert result == {"key": "value"}


class TestSarifMessageStringType:
    """SARIF message field can be a string (v2.0) or dict (v2.1+)."""

    def test_message_as_string(self):
        """SARIF result with string message doesn't crash."""
        from packages.exploitability_validation.orchestrator import convert_sarif_result
        result = {
            "ruleId": "test-rule",
            "level": "warning",
            "message": "Plain string message",
            "locations": [{
                "physicalLocation": {
                    "artifactLocation": {"uri": "test.py"},
                    "region": {"startLine": 10}
                }
            }]
        }
        finding = convert_sarif_result(result, 0, "test-tool", {}, set())
        assert finding is not None
        assert finding["message"] == "Plain string message"

    def test_message_as_dict(self):
        """SARIF result with dict message works normally."""
        from packages.exploitability_validation.orchestrator import convert_sarif_result
        result = {
            "ruleId": "test-rule",
            "level": "warning",
            "message": {"text": "Dict message text"},
            "locations": [{
                "physicalLocation": {
                    "artifactLocation": {"uri": "test.py"},
                    "region": {"startLine": 10}
                }
            }]
        }
        finding = convert_sarif_result(result, 0, "test-tool", {}, set())
        assert finding is not None
        assert finding["message"] == "Dict message text"


class TestSarifNullLocation:
    """SARIF locations array may contain null elements."""

    def test_null_location_returns_none(self):
        """SARIF result with null in locations array returns None gracefully."""
        from packages.exploitability_validation.orchestrator import convert_sarif_result
        result = {
            "ruleId": "test-rule",
            "level": "warning",
            "message": {"text": "test"},
            "locations": [None]
        }
        finding = convert_sarif_result(result, 0, "test-tool", {}, set())
        assert finding is None

    def test_non_dict_location_returns_none(self):
        """SARIF result with non-dict location returns None gracefully."""
        from packages.exploitability_validation.orchestrator import convert_sarif_result
        result = {
            "ruleId": "test-rule",
            "level": "warning",
            "message": {"text": "test"},
            "locations": ["not a dict"]
        }
        finding = convert_sarif_result(result, 0, "test-tool", {}, set())
        assert finding is None


class TestFinalizeVerdictMapping:
    """_finalize_non_memory_findings should map verdict to final_status."""

    def test_exploitable_verdict_maps_correctly(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orch = ValidationOrchestrator(config)
        findings = [
            {"id": "F-001", "vuln_type": "xss",
             "feasibility": {"status": "analyzed", "verdict": "exploitable"},
             "ruling": {"status": "confirmed"}}
        ]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "exploitable"

    def test_difficult_verdict_maps_to_constrained(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orch = ValidationOrchestrator(config)
        findings = [
            {"id": "F-001", "vuln_type": "sql_injection",
             "feasibility": {"status": "analyzed", "verdict": "difficult"},
             "ruling": {"status": "confirmed"}}
        ]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "confirmed_constrained"

    def test_unlikely_verdict_maps_to_blocked(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orch = ValidationOrchestrator(config)
        findings = [
            {"id": "F-001", "vuln_type": "xss",
             "feasibility": {"status": "analyzed", "verdict": "unlikely"},
             "ruling": {"status": "confirmed"}}
        ]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "confirmed_blocked"


class TestShouldExcludeCaseInsensitive:
    """should_exclude must be case-insensitive for path segments."""

    def test_capitalized_test_dir_excluded(self):
        assert should_exclude("src/Tests/vulnerable.py", ["tests/"])

    def test_uppercase_test_dir_excluded(self):
        assert should_exclude("src/TESTS/vulnerable.py", ["tests/"])

    def test_mixed_case_segment_match(self):
        assert should_exclude("src/Test/main.py", ["test"])

    def test_case_insensitive_basename(self):
        assert should_exclude("Makefile", ["makefile"])


class TestNoBinaryConsistentStatus:
    """Memory corruption findings without binary should be confirmed_unverified."""

    def test_no_binary_sets_confirmed_unverified(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orch = ValidationOrchestrator(config)
        orch.state.findings = {
            "stage": "D", "timestamp": "2026-01-01T00:00:00",
            "findings": [
                {"id": "F-001", "file": "a.c", "line": 1, "function": "f",
                 "vuln_type": "buffer_overflow", "status": "confirmed",
                 "final_status": "confirmed",
                 "ruling": {"status": "confirmed", "reason": "test"}}
            ]
        }
        orch._run_stage_e()
        f = orch.state.findings["findings"][0]
        assert f["final_status"] == "confirmed_unverified"


class TestFinalizeVerdictToFinalStatus:
    """Verify _finalize_non_memory_findings maps verdicts to correct final_status."""

    def _make_orchestrator(self, tmp_path):
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        return ValidationOrchestrator(config)

    def test_source_difficult_maps_to_constrained(self, tmp_path):
        """Verdict 'difficult' from source analysis → final_status 'confirmed_constrained'."""
        orch = self._make_orchestrator(tmp_path)
        findings = [{
            "vuln_type": "sql_injection",
            "feasibility": {
                "status": "pending",
                "chain_breaks": ["WAF"],
                "what_would_help": ["encoding bypass"],
            }
        }]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "confirmed_constrained"
        assert findings[0]["feasibility"]["verdict"] == "difficult"

    def test_source_unlikely_maps_to_blocked(self, tmp_path):
        """Verdict 'unlikely' from source analysis → final_status 'confirmed_blocked'."""
        orch = self._make_orchestrator(tmp_path)
        findings = [{
            "vuln_type": "sql_injection",
            "feasibility": {
                "status": "pending",
                "chain_breaks": ["WAF blocks all input"],
                "what_would_help": [],
            }
        }]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "confirmed_blocked"
        assert findings[0]["feasibility"]["verdict"] == "unlikely"

    def test_source_likely_exploitable_maps_correctly(self, tmp_path):
        """Verdict 'likely_exploitable' → final_status 'likely_exploitable'."""
        orch = self._make_orchestrator(tmp_path)
        findings = [{
            "vuln_type": "sql_injection",
            "feasibility": {
                "status": "pending",
                "chain_breaks": [],
                "what_would_help": [],
            }
        }]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "likely_exploitable"

    def test_no_source_analysis_gets_confirmed(self, tmp_path):
        """Finding with no source analysis → final_status 'confirmed' (generic)."""
        orch = self._make_orchestrator(tmp_path)
        findings = [{
            "vuln_type": "xss",
        }]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "confirmed"
        assert findings[0]["feasibility"]["status"] == "not_applicable"

    def test_does_not_overwrite_existing_analyzed_verdict(self, tmp_path):
        """Pre-analyzed finding with 'difficult' verdict → final_status 'confirmed_constrained', not 'confirmed'."""
        orch = self._make_orchestrator(tmp_path)
        findings = [{
            "vuln_type": "command_injection",
            "feasibility": {
                "status": "analyzed",
                "verdict": "difficult",
            }
        }]
        orch._finalize_non_memory_findings(findings)
        assert findings[0]["final_status"] == "confirmed_constrained"


class TestStageEErrorHandling:
    """Verify Stage E handles error verdicts and preserves analysis data."""

    def test_error_verdict_sets_error_status(self, tmp_path):
        """When analyze_binary returns error verdict, status should be 'error', not 'analyzed'."""
        from packages.exploitability_validation.orchestrator import normalize_status

        raw_verdict = normalize_status("error")
        is_error = raw_verdict == "error"
        assert is_error
        status = "error" if is_error else "analyzed"
        assert status == "error"

    def test_verdict_to_status_explicit_mappings(self, tmp_path):
        """All ExploitabilityVerdict values should have explicit mappings."""
        verdict_to_status = {
            "exploitable": "exploitable",
            "likely_exploitable": "likely_exploitable",
            "difficult": "confirmed_constrained",
            "unlikely": "confirmed_blocked",
            "unknown": "confirmed_unverified",
            "error": "confirmed_unverified",
        }
        # All possible verdicts from the enum
        for verdict in ("exploitable", "likely_exploitable", "difficult", "unlikely", "unknown"):
            assert verdict in verdict_to_status, f"Missing mapping for verdict: {verdict}"

    def test_chain_breaks_tagged_by_source(self, tmp_path):
        """Merged chain_breaks should be tagged with [source] and [binary] prefixes."""
        source_breaks = ["SQLi blocked by WAF"]
        binary_breaks = ["Full RELRO blocks GOT"]
        tagged_source = [f"[source] {b}" for b in source_breaks]
        tagged_binary = [f"[binary] {b}" for b in binary_breaks]
        merged = list(dict.fromkeys(tagged_binary + tagged_source))
        assert merged[0].startswith("[binary]")
        assert merged[1].startswith("[source]")


class TestAgentFilteringFinalStatus:
    """Verify convert_validated_to_agent_format checks final_status for ruled_out."""

    def test_final_status_ruled_out_filtered(self):
        """Finding with final_status='ruled_out' should be filtered even if status differs."""
        data = {"findings": [{
            "id": "FIND-001",
            "status": "confirmed",  # status is NOT ruled_out
            "final_status": "ruled_out",  # but final_status IS ruled_out
            "vuln_type": "buffer_overflow",
        }]}
        result = convert_validated_to_agent_format(data)
        assert len(result) == 0

    def test_missing_final_status_defaults_to_pending(self):
        """Finding without final_status should get 'pending' default."""
        data = {"findings": [{
            "id": "FIND-001",
            "vuln_type": "xss",
        }]}
        result = convert_validated_to_agent_format(data)
        assert len(result) == 1
        assert result[0]["final_status"] == "pending"

    def test_likely_exploitable_gets_error_level(self):
        """Finding with final_status='likely_exploitable' should get level='error'."""
        data = {"findings": [{
            "id": "FIND-001",
            "vuln_type": "buffer_overflow",
            "final_status": "likely_exploitable",
        }]}
        result = convert_validated_to_agent_format(data)
        assert result[0]["level"] == "error"


class TestSchemaValidationGates:
    """Verify validate_findings_for_stage catches missing fields."""

    def test_stage_b_missing_id(self):
        from packages.exploitability_validation.schemas import validate_findings_for_stage
        warnings = validate_findings_for_stage([{"vuln_type": "xss", "status": "not_disproven"}], "B")
        assert any("missing required field 'id'" in w for w in warnings)

    def test_stage_b_valid(self):
        from packages.exploitability_validation.schemas import validate_findings_for_stage
        warnings = validate_findings_for_stage([
            {"id": "FIND-001", "vuln_type": "xss", "status": "not_disproven"}
        ], "B")
        assert len(warnings) == 0

    def test_stage_e_missing_vuln_type(self):
        from packages.exploitability_validation.schemas import validate_findings_for_stage
        warnings = validate_findings_for_stage([{"id": "FIND-001"}], "E")
        assert any("vuln_type" in w for w in warnings)

    def test_non_dict_finding(self):
        from packages.exploitability_validation.schemas import validate_findings_for_stage
        warnings = validate_findings_for_stage(["not a dict"], "B")
        assert any("not a dict" in w for w in warnings)


class TestFindBinaryRecursive:
    """Verify _find_binary searches recursively and skips scripts."""

    def test_finds_nested_binary(self, tmp_path):
        """Binary in build/Release/app should be found."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)

        # Create nested executable
        nested = tmp_path / "build" / "Release"
        nested.mkdir(parents=True)
        binary = nested / "app"
        binary.write_bytes(b"#!/bin/sh\n")
        binary.chmod(0o755)

        result = orchestrator._find_binary()
        assert result is not None
        assert "app" in result

    def test_skips_python_scripts(self, tmp_path):
        """Python scripts should not be returned as binaries."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)

        # Create executable Python script
        build = tmp_path / "build"
        build.mkdir()
        script = build / "run.py"
        script.write_text("#!/usr/bin/env python3\nprint('hi')")
        script.chmod(0o755)

        result = orchestrator._find_binary()
        assert result is None  # .py should be skipped

    def test_returns_none_when_empty(self, tmp_path):
        """Empty target returns None with warning."""
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(tmp_path))
        orchestrator = ValidationOrchestrator(config)
        result = orchestrator._find_binary()
        assert result is None


class TestAttackPathRefValidation:
    """Verify Stage B validates attack_path_ref references."""

    def test_dangling_ref_logged(self, tmp_path, caplog):
        """Dangling attack_path_ref should produce a warning."""
        import logging
        import json
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        # Write attack-paths.json to disk so Stage B loads it
        with open(workdir / "attack-paths.json", "w") as f:
            json.dump([{"id": "PATH-001"}], f)

        # Set up finding with "analyzed" feasibility (won't be overwritten) and dangling ref
        orchestrator.state.findings = {
            "stage": "A",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "buffer_overflow",
                "status": "not_disproven",
                "feasibility": {
                    "status": "analyzed",
                    "attack_path_ref": "attack-paths.json#PATH-999",
                }
            }]
        }

        with caplog.at_level(logging.WARNING):
            orchestrator._run_stage_b()

        assert any("Dangling attack_path_ref" in r.message for r in caplog.records)


class TestGlibcNDisabledOptional:
    """Verify Optional[bool] semantics for glibc_n_disabled."""

    def test_default_is_none(self):
        from packages.exploit_feasibility.analyzer import FeasibilityReport
        report = FeasibilityReport()
        assert report.glibc_n_disabled is None

    def test_summary_shows_unknown_when_none(self):
        from packages.exploit_feasibility.analyzer import FeasibilityReport
        report = FeasibilityReport()
        summary = report.summary()
        assert "UNKNOWN" in summary  # n_status should show UNKNOWN

    def test_summary_shows_enabled_when_false(self):
        from packages.exploit_feasibility.analyzer import FeasibilityReport
        report = FeasibilityReport()
        report.glibc_n_disabled = False
        summary = report.summary()
        assert "ENABLED" in summary

    def test_summary_shows_disabled_when_true(self):
        from packages.exploit_feasibility.analyzer import FeasibilityReport
        report = FeasibilityReport()
        report.glibc_n_disabled = True
        summary = report.summary()
        assert "DISABLED" in summary


class TestStageDStatusConsistency:
    """Verify Stage D sets both status and final_status consistently."""

    def test_else_branch_sets_status(self, tmp_path):
        """No sanity check → both status and final_status should be 'confirmed'."""
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        orchestrator.state.findings = {
            "stage": "C",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "command_injection",
                "status": "not_disproven",
                # No sanity_check field
            }]
        }
        orchestrator._run_stage_d()
        finding = orchestrator.state.findings["findings"][0]
        assert finding["status"] == "confirmed"
        assert finding["final_status"] == "confirmed"

    def test_ruling_preserves_prior_status(self, tmp_path):
        """Ruling metadata should include prior_status for provenance."""
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        orchestrator.state.findings = {
            "stage": "C",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "command_injection",
                "status": "not_disproven",
                "sanity_check": {"passed": True}
            }]
        }
        orchestrator._run_stage_d()
        ruling = orchestrator.state.findings["findings"][0]["ruling"]
        assert ruling["prior_status"] == "not_disproven"

    def test_ruled_out_preserves_prior_status(self, tmp_path):
        """Ruled out findings should also track prior_status."""
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)
        orchestrator.state.findings = {
            "stage": "C",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "command_injection",
                "status": "not_disproven",
                "sanity_check": {"passed": False}
            }]
        }
        orchestrator._run_stage_d()
        ruling = orchestrator.state.findings["findings"][0]["ruling"]
        assert ruling["prior_status"] == "not_disproven"


class TestDanglingRefNulledOut:
    """Verify dangling attack_path_ref is nulled out after warning."""

    def test_dangling_ref_set_to_none(self, tmp_path):
        """Dangling ref should be set to None to prevent downstream failures."""
        import json
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        with open(workdir / "attack-paths.json", "w") as f:
            json.dump([{"id": "PATH-001"}], f)

        orchestrator.state.findings = {
            "stage": "A",
            "timestamp": "2026-01-01T00:00:00",
            "findings": [{
                "id": "FIND-0001",
                "vuln_type": "buffer_overflow",
                "status": "not_disproven",
                "feasibility": {
                    "status": "analyzed",
                    "attack_path_ref": "attack-paths.json#PATH-999",
                }
            }]
        }

        orchestrator._run_stage_b()
        ref = orchestrator.state.findings["findings"][0]["feasibility"]["attack_path_ref"]
        assert ref is None


class TestUnexpectedVerdictLogged:
    """Verify unexpected verdicts are logged before defaulting."""

    def test_none_verdict_logged(self, tmp_path, caplog):
        """None/unexpected verdict should produce a warning."""
        import logging
        workdir = tmp_path / "out"
        workdir.mkdir()
        config = PipelineConfig(target_path=str(tmp_path), workdir=str(workdir))
        orchestrator = ValidationOrchestrator(config)

        verdict_to_status = {
            "exploitable": "exploitable",
            "likely_exploitable": "likely_exploitable",
            "difficult": "confirmed_constrained",
            "unlikely": "confirmed_blocked",
            "unknown": "confirmed_unverified",
            "error": "confirmed_unverified",
        }
        # Verify that an unexpected value would fall through
        assert "banana" not in verdict_to_status
        assert verdict_to_status.get("banana", "confirmed_unverified") == "confirmed_unverified"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
